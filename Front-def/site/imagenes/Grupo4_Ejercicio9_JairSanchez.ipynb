{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1894ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "use strict;\n",
    "use warnings;\n",
    "use Data::Dump qw(dump);\n",
    "use strict;\n",
    "use warnings;\n",
    "use Math::Complex;\n",
    "use List::Util qw(sum);\n",
    "use sml; # Statistical Machine Learning Library\n",
    "use Chart::Plotly::Plot;\n",
    "use Chart::Plotly::Trace::Scatter;\n",
    "IPerl->load_plugin('Chart::Plotly');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f2e32",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85c916",
   "metadata": {},
   "source": [
    "A simple but powerful approach for making predictions is to use the most similar historical\n",
    "examples to the new data. This is the principle behind the k-Nearest Neighbors algorithm. In\n",
    "this tutorial you will discover how to implement the k-Nearest Neighbors algorithm from scratch\n",
    "with Python. After completing this tutorial you will know:\n",
    " How to calculate the similarity between two pieces of data.\n",
    " How to make a prediction using the most similar historical records.\n",
    " How to use k-Nearest Neighbors for both classification and regression problems.\n",
    "Let’s get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2046d4",
   "metadata": {},
   "source": [
    "# 13.1 Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb8569",
   "metadata": {},
   "source": [
    "This section will provide a brief background on the k-Nearest Neighbors algorithm that we will\n",
    "implement in this tutorial and the Abalone dataset to which we will apply it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9362da8",
   "metadata": {},
   "source": [
    "# 13.1.1 k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed29cdb",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors algorithm or KNN for short is a very simple technique. The entire\n",
    "training dataset is stored. When a prediction is required, the k-most similar records to a\n",
    "new record from the training dataset are then located. From these neighbors, a summarized\n",
    "prediction is made. Similarity between records can be measured many different ways. A problem\n",
    "or data-specific method can be used. Generally, with tabular data, a good starting point is the\n",
    "Euclidean distance.\n",
    "Once the neighbors are discovered, the summary prediction can be made by returning the\n",
    "most common outcome or taking the average. As such, KNN can be used for classification\n",
    "or regression problems. There is no model to speak of other than holding the entire training\n",
    "dataset. Because no work is done until a prediction is required, KNN is often referred to as a\n",
    "lazy learning method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2729ad",
   "metadata": {},
   "source": [
    "# 13.1.2 Abalone Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48930a8",
   "metadata": {},
   "source": [
    "In this tutorial we will use the Abalone Dataset. This dataset involves the prediction of the age\n",
    "of abalone. The baseline performance on the problem is approximately 16% or an RMSE of\n",
    "approximately 3.2 rings. You can learn more about it in Appendix A, Section A.8. Download\n",
    "the dataset and save it into your current working directory with the filename abalone.csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b923bfa",
   "metadata": {},
   "source": [
    "# 13.2 Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb9e7c",
   "metadata": {},
   "source": [
    "This tutorial is broken down into 5 parts:\n",
    "1. Euclidean Distance.\n",
    "2. Get Neighbors.\n",
    "3. Make Predictions.\n",
    "4. Abalone Case Study as Classification.\n",
    "5. Abalone Case Study as Regression.\n",
    "These steps will teach you the fundamentals of implementing and applying the k-Nearest\n",
    "Neighbors algorithm for classification and regression predictive modeling problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839a89b",
   "metadata": {},
   "source": [
    "# 13.2.1 Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee1afe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The first step needed is to calculate the distance between two rows in a dataset. Rows of data are mostly made up of numbers, and an easy way to calculate the distance between two rows or vectors of numbers is to draw a straight line. This makes sense in 2D or 3D and scales nicely to higher dimensions. We can calculate the straight line distance between two vectors using the Euclidean distance measure. It is calculated as the square root of the sum of the squared differences between the two vectors:\n",
    "\n",
    "\n",
    "\\text{{distance}} = \\sqrt{\\sum_{i=1}^{n}(x_{1i} - x_{2i})^2} \\quad (13.1)\n",
    "\n",
    "\n",
    "Where \\(x_1\\) is the first row of data, \\(x_2\\) is the second row of data, and \\(i\\) is the index to a specific column as we sum across all columns. With Euclidean distance, the smaller the value, the more similar two records will be. A value of 0 means that there is no difference between two records. Below is a function named \\texttt{euclidean\\_distance()} that implements this in Python.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85182eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::euclidean_distance"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $euclidean_distance = sub{\n",
    "    my ($self, $row1, $row2) = @_;\n",
    "    my $distance = 0.0;\n",
    "    for my $i (0 .. $#{$row1} - 1) {\n",
    "        $distance += ($row1->[$i] - $row2->[$i])**2;\n",
    "    }\n",
    "    return sqrt($distance);\n",
    "};\n",
    "sml->add_to_class('sml','euclidean_distance',$euclidean_distance);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec4963",
   "metadata": {},
   "source": [
    "You can see that the function assumes that the last column in each row is an output value\n",
    "which is ignored from the distance calculation. We can test this distance function with a small\n",
    "contrived classification dataset. We will use this dataset a few times as we construct the elements\n",
    "needed for the KNN algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67c04769",
   "metadata": {},
   "source": [
    "# X1 X2 Y\n",
    "2.7810836 2.550537003 0\n",
    "1.465489372 2.362125076 0\n",
    "3.396561688 4.400293529 0\n",
    "1.38807019 1.850220317 0\n",
    "3.06407232 3.005305973 0\n",
    "7.627531214 2.759262235 1\n",
    "5.332441248 2.088626775 1\n",
    "6.922596716 1.77106367 1\n",
    "8.675418651 -0.242068655 1\n",
    "7.673756466 3.508563011 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e32db2",
   "metadata": {},
   "source": [
    "Below is a plot of the dataset using different colors to show the different classes for each\n",
    "point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df50d1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "//# sourceURL=iperl-devel-plugin-chart-plotly.js\n",
       "            $('#Plotly').each(function(i, e) { $(e).attr('id', 'plotly') });\n",
       "\n",
       "            if (!window.Plotly) {\n",
       "                requirejs.config({\n",
       "                  paths: {\n",
       "                    plotly: ['https://cdn.plot.ly/plotly-latest.min']},\n",
       "                });\n",
       "                window.Plotly = {\n",
       "react : function (div, data, layout, config){\n",
       "                    require(['plotly'], function(plotly) {\n",
       "                      window.Plotly=plotly;\n",
       "Plotly.react(div, data, layout, config);                    });\n",
       "                  }\n",
       "                }\n",
       "            }\n",
       "</script>\n",
       "<div id=\"91798f0f-c13f-11ee-900c-90e983289d79\"></div>\n",
       "\n",
       "<script>\n",
       "Plotly.react(document.getElementById('91798f0f-c13f-11ee-900c-90e983289d79'),[{\"mode\":\"markers\",\"type\":\"scatter\",\"y\":[2.550537003,2.362125076,4.400293529,1.850220317,3.005305973,2.759262235,2.088626775,1.77106367,-0.242068655,3.508563011],\"x\":[2.7810836,1.465489372,3.396561688,1.38807019,3.06407232,7.627531214,5.332441248,6.922596716,8.675418651,7.673756466],\"marker\":{\"size\":10,\"color\":[\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\"],\"symbol\":[\"diamond\",\"diamond\",\"diamond\",\"diamond\",\"diamond\",\"square\",\"square\",\"square\",\"square\",\"square\"]}}] ,{\"title\":{\"text\":\"Conjunto de Datos para Prueba de Regresión Logística\"}} );\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Datos del conjunto de datos\n",
    "my ($X1, $X2, $Y) = (\n",
    "    [2.7810836, 1.465489372, 3.396561688, 1.38807019, 3.06407232, 7.627531214, 5.332441248, 6.922596716, 8.675418651, 7.673756466],\n",
    "    [2.550537003, 2.362125076, 4.400293529, 1.850220317, 3.005305973, 2.759262235, 2.088626775, 1.77106367, -0.242068655, 3.508563011],\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    ");\n",
    "\n",
    "# Colores y símbolos para las clases\n",
    "my @colors = map { $_ == 0 ? 'red' : 'blue' } @$Y;\n",
    "my @symbols = map { $_ == 0 ? 'diamond' : 'square' } @$Y;\n",
    "\n",
    "# Tamaño del símbolo (ajústalo según sea necesario)\n",
    "my $symbol_size = 10;\n",
    "\n",
    "# Crear el rastro (trace) Scatter\n",
    "my $scatter_trace = Chart::Plotly::Trace::Scatter->new(\n",
    "    x      => $X1,\n",
    "    y      => $X2,\n",
    "    mode   => 'markers',\n",
    "    marker => { color => \\@colors, size => $symbol_size, symbol => \\@symbols }\n",
    ");\n",
    "\n",
    "# Crear el gráfico\n",
    "my $plot = Chart::Plotly::Plot->new(\n",
    "    traces => [$scatter_trace],\n",
    "    layout => { title => { text => 'Conjunto de Datos para Prueba de Regresión Logística' } }\n",
    ");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80639d64",
   "metadata": {},
   "source": [
    "Putting this all together, we can write a small example to test our distance function by\n",
    "printing the distance between the first row and all other rows. We would expect the distance\n",
    "between the first row and itself to be 0, a good thing to look out for. The full example is listed\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4454fad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.32901739152758\n",
      "1.94946466556532\n",
      "1.55914393855405\n",
      "0.535628072193849\n",
      "4.85094018698641\n",
      "2.59283375995051\n",
      "4.21422704263287\n",
      "6.52240998822834\n",
      "4.98558538244979\n"
     ]
    }
   ],
   "source": [
    "# Conjunto de datos\n",
    "my $dataset = [\n",
    "    [2.7810836, 2.550537003, 0],\n",
    "    [1.465489372, 2.362125076, 0],\n",
    "    [3.396561688, 4.400293529, 0],\n",
    "    [1.38807019, 1.850220317, 0],\n",
    "    [3.06407232, 3.005305973, 0],\n",
    "    [7.627531214, 2.759262235, 1],\n",
    "    [5.332441248, 2.088626775, 1],\n",
    "    [6.922596716, 1.77106367, 1],\n",
    "    [8.675418651, -0.242068655, 1],\n",
    "    [7.673756466, 3.508563011, 1]\n",
    "];\n",
    "\n",
    "# Seleccionar la primera fila como referencia\n",
    "my $row0 = $dataset->[0];\n",
    "\n",
    "# Calcular la distancia para cada fila en el conjunto de datos\n",
    "for my $row (@$dataset) {\n",
    "    my $distance = sml->euclidean_distance($row0, $row);\n",
    "    print(\"$distance\\n\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e05fa7",
   "metadata": {},
   "source": [
    "Running this example prints the distances between the first row and every row in the dataset,\n",
    "including itself."
   ]
  },
  {
   "cell_type": "raw",
   "id": "19a95aea",
   "metadata": {},
   "source": [
    "0.0\n",
    "1.32901739153\n",
    "1.94946466557\n",
    "1.55914393855\n",
    "0.535628072194\n",
    "4.85094018699\n",
    "2.59283375995\n",
    "4.21422704263\n",
    "6.52240998823\n",
    "4.98558538245\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6cc2b",
   "metadata": {},
   "source": [
    "Now it is time to use the distance calculation to locate neighbors within a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50940ce",
   "metadata": {},
   "source": [
    "# 13.2.2 Get Neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6963a",
   "metadata": {},
   "source": [
    "Neighbors for a new piece of data in the dataset are the k closest instances, as defined by our\n",
    "distance measure. To locate the neighbors for a new piece of data within a dataset we must\n",
    "first calculate the distance between each record in the dataset to the new piece of data. We can\n",
    "do this using our distance function above. Once distances are calculated, we must sort all of the\n",
    "records in the training dataset by their distance to the new data. We can then select the top k\n",
    "to return as the most similar neighbors.\n",
    "We can do this by keeping track of the distance for each record in the dataset as a tuple,\n",
    "sort the list of tuples by the distance (in descending order) and then retrieve the neighbors.\n",
    "Below is a function named get neighbors() that implements this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74abab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::get_neighbors"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locate the most similar neighbors\n",
    "my $get_neighbors = sub{\n",
    "    my ($self, $train, $test_row, $num_neighbors) = @_;\n",
    "    my @distances;\n",
    "    for my $train_row (@$train) {\n",
    "        my $dist = sml->euclidean_distance($test_row, $train_row);\n",
    "        push @distances, [$train_row, $dist];\n",
    "    }\n",
    "    @distances = sort { $a->[1] <=> $b->[1] } @distances;\n",
    "    my @neighbors;\n",
    "    for my $i (0 .. $num_neighbors - 1) {\n",
    "        push @neighbors, $distances[$i][0];\n",
    "    }\n",
    "    return \\@neighbors;\n",
    "};\n",
    "sml->add_to_class('sml','get_neighbors',$get_neighbors);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca002218",
   "metadata": {},
   "source": [
    "You can see that the euclidean distance() function developed in the previous step is used\n",
    "to calculate the distance between each train row and the new test row. The list of train row\n",
    "and distance tuples is sorted where a custom key is used ensuring that the second item in the\n",
    "tuple (tup[1]) is used in the sorting operation.\n",
    "Finally, a list of the num neighbors most similar neighbors to test row is returned. We\n",
    "can test this function with the small contrived dataset prepared in the previous section. The\n",
    "complete example is listed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ff3e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7810836 2.550537003 0\n",
      "3.06407232 3.005305973 0\n",
      "1.465489372 2.362125076 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Conjunto de datos\n",
    "my $dataset = [\n",
    "    [2.7810836, 2.550537003, 0],\n",
    "    [1.465489372, 2.362125076, 0],\n",
    "    [3.396561688, 4.400293529, 0],\n",
    "    [1.38807019, 1.850220317, 0],\n",
    "    [3.06407232, 3.005305973, 0],\n",
    "    [7.627531214, 2.759262235, 1],\n",
    "    [5.332441248, 2.088626775, 1],\n",
    "    [6.922596716, 1.77106367, 1],\n",
    "    [8.675418651, -0.242068655, 1],\n",
    "    [7.673756466, 3.508563011, 1]\n",
    "];\n",
    "\n",
    "# Obtener vecinos para la primera instancia del conjunto de datos\n",
    "my $neighbors = sml->get_neighbors($dataset, $dataset->[0], 3);\n",
    "\n",
    "# Imprimir los vecinos\n",
    "for my $neighbor (@$neighbors) {\n",
    "    print(\"@$neighbor\\n\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1750bd",
   "metadata": {},
   "source": [
    "Running this example prints the 3 most similar records in the dataset to the first record, in\n",
    "order of similarity. As expected, the first record is the most similar to itself and is at the top of\n",
    "the list."
   ]
  },
  {
   "cell_type": "raw",
   "id": "762c5c12",
   "metadata": {},
   "source": [
    "[2.7810836, 2.550537003, 0]\n",
    "[3.06407232, 3.005305973, 0]\n",
    "[1.465489372, 2.362125076, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2114dd6",
   "metadata": {},
   "source": [
    "Now that we know how to get neighbors from the dataset, we can use them to make\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75da3fb9",
   "metadata": {},
   "source": [
    "### 13.2.3 Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58a291",
   "metadata": {},
   "source": [
    "The most similar neighbors collected from the training dataset can be used to make predictions.\n",
    "In the case of classification, we can return the most represented class among the neighbors.\n",
    "We can achieve this by performing the max() function on the list of output values from the\n",
    "neighbors. Given a list of class values observed in the neighbors, the max() function takes a set\n",
    "of unique class values and calls the count on the list of class values for each class value in the\n",
    "set. Below is the function named predict classification() that implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a053eb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::predict_classification"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subrutina para predecir la clasificación con vecinos\n",
    "my $predict_classification = sub{\n",
    "    my ($self, $train, $test_row, $num_neighbors) = @_;\n",
    "    my $neighbors = get_neighbors($train, $test_row, $num_neighbors);\n",
    "    my @output_values = map { $_->[-1] } @$neighbors;\n",
    "    my $prediction = max(keys %{{ map { $_ => 1 } @output_values }});\n",
    "    return $prediction;\n",
    "};\n",
    "sml->add_to_class('sml','predict_classification',$predict_classification);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc0fca",
   "metadata": {},
   "source": [
    "We can test this function on the above contrived dataset. Below is a complete example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b2fca5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Undefined subroutine &main::get_neighbors called at reply input line 4.\n",
     "output_type": "error",
     "traceback": [
      "Undefined subroutine &main::get_neighbors called at reply input line 4.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Conjunto de datos\n",
    "my $dataset = [\n",
    "    [2.7810836, 2.550537003, 0],\n",
    "    [1.465489372, 2.362125076, 0],\n",
    "    [3.396561688, 4.400293529, 0],\n",
    "    [1.38807019, 1.850220317, 0],\n",
    "    [3.06407232, 3.005305973, 0],\n",
    "    [7.627531214, 2.759262235, 1],\n",
    "    [5.332441248, 2.088626775, 1],\n",
    "    [6.922596716, 1.77106367, 1],\n",
    "    [8.675418651, -0.242068655, 1],\n",
    "    [7.673756466, 3.508563011, 1]\n",
    "];\n",
    "\n",
    "# Hacer una predicción de clasificación para la primera instancia del conjunto de datos\n",
    "my $prediction = sml->predict_classification($dataset, $dataset->[0], 3);\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(\"Expected $dataset->[0][-1], Got $prediction.\\n\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e9d68d",
   "metadata": {},
   "source": [
    "Running this example prints the expected classification of 0 and the actual classification\n",
    "predicted from the 3 most similar neighbors in the dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "640a2460",
   "metadata": {},
   "source": [
    "Expected 0, Got 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee588469",
   "metadata": {},
   "source": [
    "We can imagine how the predict classification() function can be changed to calculate\n",
    "the mean value of the outcome values. Below is a quick example named predict regression()\n",
    "that we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac1ded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "*sml::predict_regression"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $predict_regression = sub{\n",
    "    my ($self, $train, $test_row, $num_neighbors) = @_;\n",
    "    my $neighbors = sml->get_neighbors($train, $test_row, $num_neighbors);\n",
    "    my @output_values = map $_->[-1], @$neighbors;\n",
    "    my $prediction = sum(@output_values) / @$neighbors;\n",
    "    return $prediction;\n",
    "};\n",
    "sml->add_to_class('sml','predict_regression',$predict_regression);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac8e9b8",
   "metadata": {},
   "source": [
    "We now have all of the pieces to make predictions with KNN. Let’s apply it to the Abalone\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7b4a2",
   "metadata": {},
   "source": [
    "# 13.2.4 Abalone Case Study as Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a796d65",
   "metadata": {},
   "source": [
    "In this section we will apply the k-Nearest Neighbors algorithm to the Abalone dataset. The\n",
    "first step is to load the dataset and convert the loaded data to numbers that we can use with\n",
    "the Euclidean distance calculation. For this we will use the helper function load csv() to load\n",
    "the file, str column to float() to convert string numbers to floats and str column to int()\n",
    "to convert the sex column (column 0) to integer values.\n",
    "We will evaluate the algorithm using k-fold cross-validation with 5 folds. This means that\n",
    "4177\n",
    "5 = 835.4 or just over 830 records will be in each fold. We will use the helper functions evaluate algorithm() to evaluate the algorithm with cross-validation and accuracy metric()\n",
    "to calculate the accuracy of predictions. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af6bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 22.874251497006, 22.7544910179641, 26.2275449101796, 25.1497005988024, 22.9940119760479\n",
      "Mean Accuracy: 24.000%\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "Warning",
     "evalue": "Subroutine sml::cross_validation_split redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n\nSubroutine sml::accuracy_metric redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n\nSubroutine sml::evaluate_algorithm redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n\nSubroutine sml::predict_classification redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine sml::cross_validation_split redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n\nSubroutine sml::accuracy_metric redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n\nSubroutine sml::evaluate_algorithm redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n\nSubroutine sml::predict_classification redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split a dataset into k folds\n",
    "#igual\n",
    "my $cross_validation_split = sub{\n",
    "    my ($self, $dataset, $n_folds) = @_;\n",
    "    my @dataset_split;\n",
    "    my @dataset_copy = @$dataset;\n",
    "    my $fold_size = int(@$dataset / $n_folds);\n",
    "    for (1 .. $n_folds) {\n",
    "        my @fold;\n",
    "        while (@fold < $fold_size) {\n",
    "            my $index = int(rand(@dataset_copy));\n",
    "            push @fold, splice @dataset_copy, $index, 1;\n",
    "        }\n",
    "        push @dataset_split, \\@fold;\n",
    "    }\n",
    "    return \\@dataset_split;\n",
    "};\n",
    "\n",
    "sml->add_to_class('sml','cross_validation_split',$cross_validation_split);\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "#No igual\n",
    "my $accuracy_metric = sub{\n",
    "    my ($self, $actual, $predicted) = @_;\n",
    "    my $correct = 0;\n",
    "    for my $i (0 .. @$actual - 1) {\n",
    "        $correct++ if $actual->[$i] == $predicted->[$i];\n",
    "    }\n",
    "    return $correct / @$actual * 100.0;\n",
    "};\n",
    "sml->add_to_class('sml','accuracy_metric',$accuracy_metric);\n",
    "\n",
    "\n",
    "# Evaluate an algorithm using a cross-validation split\n",
    "#No igual\n",
    "my $evaluate_algorithm = sub{\n",
    "    my ($self, $dataset, $algorithm, $n_folds, @args) = @_;\n",
    "    my $folds = sml->cross_validation_split($dataset, $n_folds);\n",
    "    my @scores;\n",
    "    for my $fold (@$folds) {\n",
    "        my @train_set = @$folds;\n",
    "        @train_set = grep { $_ ne $fold } @train_set;\n",
    "        @train_set = map @$_, @train_set;\n",
    "        my @test_set;\n",
    "        for my $row (@$fold) {\n",
    "            my @row_copy = @$row;\n",
    "            push @test_set, \\@row_copy;\n",
    "            $row_copy[-1] = undef;\n",
    "        }\n",
    "        my $predicted = $algorithm->(\\@train_set, \\@test_set, @args);\n",
    "        my @actual = map $_->[-1], @$fold;\n",
    "        my $accuracy = sml->accuracy_metric(\\@actual, $predicted);\n",
    "        push @scores, $accuracy;\n",
    "    }\n",
    "    return \\@scores;\n",
    "};\n",
    "sml->add_to_class('sml','evaluate_algorithm',$evaluate_algorithm);\n",
    "\n",
    "#No igual\n",
    "my $predict_classification = sub{\n",
    "    my ($self, $train, $test_row, $num_neighbors) = @_;\n",
    "    my $neighbors = sml->get_neighbors($train, $test_row, $num_neighbors);\n",
    "    my @output_values = map $_->[-1], @$neighbors;\n",
    "    my %counts;\n",
    "    $counts{$_}++ for @output_values;\n",
    "    my $prediction = (sort { $counts{$b} <=> $counts{$a} } keys %counts)[0];\n",
    "    return $prediction;\n",
    "};\n",
    "sml->add_to_class('sml','predict_classification',$predict_classification);\n",
    "\n",
    "# kNN Algorithm\n",
    "#igual\n",
    "sub k_nearest_neighbors {\n",
    "    my ($train, $test, $num_neighbors) = @_;\n",
    "    my @predictions;\n",
    "    for my $row (@$test) {\n",
    "        my $output = sml->predict_classification($train, $row, $num_neighbors);\n",
    "        push @predictions, $output;\n",
    "    }\n",
    "    return \\@predictions;\n",
    "}\n",
    "\n",
    "# Test the kNN on the Abalone dataset\n",
    "srand(1);\n",
    "\n",
    "# load and prepare data\n",
    "my $filename = 'abalone.csv';\n",
    "my $dataset = sml->load_csv($filename);\n",
    "for my $i (1 .. $#{$dataset->[0]}) {\n",
    "    sml->str_column_to_float($dataset, $i);\n",
    "}\n",
    "\n",
    "# convert the first column to integers\n",
    "my $lookup = sml->str_column_to_int($dataset, 0);\n",
    "\n",
    "# evaluate algorithm\n",
    "my $n_folds = 5;\n",
    "my $num_neighbors = 5;\n",
    "my $scores = sml->evaluate_algorithm($dataset, \\&k_nearest_neighbors, $n_folds, $num_neighbors);\n",
    "print 'Scores: ' . join(', ', @$scores) . \"\\n\";\n",
    "printf 'Mean Accuracy: %.3f%%\\n', sum(@$scores) / @$scores;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f798d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8280df4f",
   "metadata": {},
   "source": [
    "A value of k=5 neighbors was used to make predictions. You may experiment with larger k\n",
    "values to increase accuracy. Running the example prints the accuracy scores achieved on each\n",
    "fold and the mean of those scores. We can see that the mean accuracy of 23% is better than the\n",
    "baseline of 16%, but is quite poor in general. This is because of the large number of classes\n",
    "making accuracy a poor judge of skill on this problem. This fact, combined with the fact that\n",
    "many of the classes have few or one example also makes the problem challenging."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c66620ec",
   "metadata": {},
   "source": [
    "Scores: [24.191616766467067, 24.431137724550897, 23.47305389221557, 22.035928143712574,\n",
    "23.11377245508982]\n",
    "Mean Accuracy: 23.449%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49bd629",
   "metadata": {},
   "source": [
    "We can also model the dataset as a regression predictive modeling problem. This is because\n",
    "the class values have a natural ordinal relationship as they are years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334dfac",
   "metadata": {},
   "source": [
    "# Abalone Case Study as Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510defd8",
   "metadata": {},
   "source": [
    "Regression may be a more useful way to model this problem given the large number of classes\n",
    "and sparseness of some class values. We can easily change our above example to regression by\n",
    "changing KNN to predict the mean of the neighbors and using Root Mean Squared Error to\n",
    "evaluate predictions. Below is a complete working example with these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ef322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 2.53596879408066, 2.76071673415212, 2.54045118346503, 2.78404349886865, 2.69085833199636\n",
      "Mean RMSE: 2.662\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "Warning",
     "evalue": "Subroutine sml::rmse_metric redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n\nSubroutine sml::evaluate_algorithm redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n",
     "output_type": "error",
     "traceback": [
      "Subroutine sml::rmse_metric redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n\nSubroutine sml::evaluate_algorithm redefined at /usr/local/lib/perl5/site_perl/5.32.1/x86_64-linux/sml.pm line 19.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate root mean squared error\n",
    "my $rmse_metric = sub{\n",
    "    my ($self, $actual, $predicted) = @_;\n",
    "    my $sum_error = 0.0;\n",
    "    for my $i (0 .. @$actual - 1) {\n",
    "        my $prediction_error = $predicted->[$i] - $actual->[$i];\n",
    "        $sum_error += ($prediction_error ** 2);\n",
    "    }\n",
    "    my $mean_error = $sum_error / @$actual;\n",
    "    return sqrt($mean_error);\n",
    "};\n",
    "sml->add_to_class('sml','rmse_metric',$rmse_metric);\n",
    "\n",
    "# Evaluate an algorithm using a cross-validation split\n",
    "\n",
    "my $evaluate_algorithm = sub{\n",
    "    my ($self, $dataset, $algorithm, $n_folds, @args) = @_;\n",
    "    my $folds = sml->cross_validation_split($dataset, $n_folds);\n",
    "    my @scores;\n",
    "    for my $fold (@$folds) {\n",
    "        my @train_set = @$folds;\n",
    "        @train_set = grep { $_ ne $fold } @train_set;\n",
    "        @train_set = map @$_, @train_set;\n",
    "        my @test_set;\n",
    "        for my $row (@$fold) {\n",
    "            my $row_copy = [@$row];\n",
    "            push @test_set, $row_copy;\n",
    "            $row_copy->[-1] = undef;\n",
    "        }\n",
    "        my $predicted = $algorithm->(\\@train_set, \\@test_set, @args);\n",
    "        my @actual = map $_->[-1], @$fold;\n",
    "        my $rmse = sml->rmse_metric(\\@actual, $predicted);\n",
    "        push @scores, $rmse;\n",
    "    }\n",
    "    return \\@scores;\n",
    "};\n",
    "sml->add_to_class('sml','evaluate_algorithm',$evaluate_algorithm);\n",
    "\n",
    "# Test the kNN on the Abalone dataset\n",
    "srand(1);\n",
    "\n",
    "# load and prepare data\n",
    "my $filename = 'abalone.csv';\n",
    "my $dataset = sml->load_csv($filename);\n",
    "shift @$dataset;\n",
    "for my $i (1 .. $#{$dataset->[0]}) {\n",
    "    sml->str_column_to_float($dataset, $i);\n",
    "}\n",
    "\n",
    "# convert first column to integers\n",
    "my $lookup = sml->str_column_to_int($dataset, 0);\n",
    "\n",
    "# evaluate algorithm\n",
    "my $n_folds = 5;\n",
    "my $num_neighbors = 5;\n",
    "my $scores = sml->evaluate_algorithm($dataset, \\&k_nearest_neighbors, $n_folds, $num_neighbors);\n",
    "print 'Scores: ' . join(', ', @$scores) . \"\\n\";\n",
    "printf 'Mean RMSE: %.3f\\n', sum(@$scores) / @$scores;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b15c91",
   "metadata": {},
   "source": [
    "Running this example prints the RMSE on each fold and the mean RMSE across all folds.\n",
    "We can see that the RMSE of 2.242 rings is better than the baseline of 3.222 rings. We also\n",
    "have a model that is perhaps more useful in the domain with an performance that is easier to\n",
    "understand.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "403c5935",
   "metadata": {},
   "source": [
    "Scores: [2.1235153320071554, 2.258503558410589, 2.2739767060988636, 2.3582090027389,\n",
    "2.196633243122396]\n",
    "Mean RMSE: 2.242"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c1e8c",
   "metadata": {},
   "source": [
    " # 13.3 Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a580e1",
   "metadata": {},
   "source": [
    "This section lists extensions to the tutorial you may wish to consider to investigate.\n",
    "* Tune KNN. Try larger and larger k values to see if you can improve the performance of\n",
    "the algorithm on the Abalone dataset.\n",
    "* Regression for Classification. Combine the approach used to make predictions for\n",
    "regression problems (take the mean) with the classification approach to making predictions\n",
    "(return an integer) and see if you can improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c7ae6",
   "metadata": {},
   "source": [
    "* More Distance Measures. Implement other distance measures that you can use to find\n",
    "similar historical data, such as Hamming distance, Manhattan distance and Minkowski\n",
    "distance.\n",
    "* Data Preparation. Distance measures are strongly affected by the scale of the input\n",
    "data. Experiment with normalization and standardization data preparation methods in\n",
    "order to improve results.\n",
    "* More Problems. As always, experiment with the technique on more and different\n",
    "classification and regression problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ae6be",
   "metadata": {},
   "source": [
    "# 13.4 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a681df",
   "metadata": {},
   "source": [
    "In this tutorial you discovered how to implement the k-Nearest Neighbors algorithm from scratch\n",
    "with Python. Specifically, you learned:\n",
    "* How to implement k-Nearest Neighbors from scratch in Python.\n",
    "* How to apply k-Nearest Neighbors to classification problems.\n",
    "* How to apply k-Nearest Neighbors to regression problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbfd4f",
   "metadata": {},
   "source": [
    "## 13.4.1 Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c9937",
   "metadata": {},
   "source": [
    "* Section 3.5 Comparison of Linear Regression with K-Nearest Neighbors, page 104, An\n",
    "Introduction to Statistical Learning, 2014.\n",
    "http://amzn.to/2eeTyQX\n",
    "* Section 18.8. Nonparametric Models, page 737, Artificial Intelligence: A Modern Approach,\n",
    "2010.\n",
    "http://amzn.to/2e3lFqP\n",
    "* Section 7.4 K-Nearest Neighbors, page 159, and Section 13.5 K-Nearest Neighbors, page\n",
    "350 Applied Predictive Modeling, 2013\n",
    "http://amzn.to/2e3lNXF\n",
    "* Section 4.7, Instance-based learning, page 128, Data Mining: Practical Machine Learning\n",
    "Tools and Techniques, second edition, 2005.\n",
    "http://amzn.to/2fj3SYY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7bcda",
   "metadata": {},
   "source": [
    "## 13.4.2 Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed202f9",
   "metadata": {},
   "source": [
    "In the next tutorial, you will discover how to implement and apply the Learning Vector\n",
    "Optimization algorithm for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3e3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPerl 0.011",
   "language": "perl",
   "name": "iperl"
  },
  "language_info": {
   "file_extension": ".pl",
   "mimetype": "text/x-perl",
   "name": "perl",
   "version": "5.32.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
